"""
End-to-end validation script for the Multi-Modal Health Insights Platform.

This script demonstrates how to:
1. Load all three data modalities (tabular, image, text)
2. Preprocess each modality with appropriate preprocessors
3. Create PyTorch datasets for each modality
"""

import os
import logging
import numpy as np
import pandas as pd
import torch
from torch.utils.data import DataLoader
from typing import Dict, List, Tuple, Any

# Set up logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Import data loaders
from data.data_loader import load_clinical_tabular, load_chestxray_images, load_mtsamples_text

# Import preprocessors
from preprocessing.clinical_preprocessor import TabularPreprocessor
from preprocessing.image_preprocessor import ImagePreprocessor, ChestXrayDataset
from preprocessing.text_preprocessor import TextPreprocessor, MTSamplesTextDataset

def validate_tabular():
    """Validate tabular data loading and preprocessing"""
    print("\n" + "="*80)
    print("VALIDATING TABULAR DATA")
    print("="*80)
    
    # 1. Load data
    df = load_clinical_tabular()
    print(f"✓ Loaded tabular data: {df.shape[0]} samples, {df.shape[1]} features")
    print(f"✓ Feature names: {', '.join(df.columns.tolist())}")
    print(f"✓ First 3 rows:\n{df.head(3)}")
    
    # 2. Create preprocessor
    config = {
        'impute_strategy': 'mean',
        'categorical_encoding': 'one-hot',
        'normalization': 'standard'
    }
    preprocessor = TabularPreprocessor(config)
    
    # 3. Split features and target
    X = df.drop('target', axis=1)
    y = df['target']
    
    # 4. Preprocess features
    X_processed = preprocessor.fit_transform(X)
    feature_names = preprocessor.get_feature_names()
    
    print(f"✓ Preprocessed data shape: {X_processed.shape}")
    print(f"✓ First 5 feature names: {', '.join(feature_names[:5])}")
    
    # 5. Convert to PyTorch tensors
    X_tensor = torch.tensor(X_processed, dtype=torch.float32)
    y_tensor = torch.tensor(y.values, dtype=torch.long)
    
    print(f"✓ Feature tensor shape: {X_tensor.shape}")
    print(f"✓ Target tensor shape: {y_tensor.shape}")
    
    return {
        "data": df,
        "preprocessor": preprocessor,
        "X_processed": X_processed,
        "X_tensor": X_tensor,
        "y_tensor": y_tensor
    }

def validate_image():
    """Validate image data loading and preprocessing"""
    print("\n" + "="*80)
    print("VALIDATING IMAGE DATA")
    print("="*80)
    
    # 1. Load data
    try:
        image_data, image_paths = load_chestxray_images(sample_size=10)
        print(f"✓ Loaded image metadata: {len(image_data)} samples")
        
        if len(image_paths) > 0:
            print(f"✓ Found {len(image_paths)} valid image paths")
            
            # 2. Create preprocessor
            config = {
                'resize': [224, 224],
                'normalize': True,
                'mean': [0.5],
                'std': [0.5]
            }
            preprocessor = ImagePreprocessor(config)
            
            # 3. Create dataset with dummy labels
            dummy_labels = [1] * len(image_paths)  # Just for demonstration
            dataset = ChestXrayDataset(
                image_paths=image_paths,
                labels=dummy_labels,
                preprocessor=preprocessor
            )
            
            print(f"✓ Created dataset with {len(dataset)} images")
            
            # 4. Test loading a batch
            dataloader = DataLoader(dataset, batch_size=2, shuffle=False)
            batch = next(iter(dataloader), None)
            
            if batch:
                images, labels = batch
                print(f"✓ Batch shape: {images.shape}")
                print(f"✓ Labels shape: {labels.shape}")
                
                return {
                    "metadata": image_data,
                    "preprocessor": preprocessor,
                    "dataset": dataset,
                    "sample_tensor": images[0]
                }
        
        print("✗ No image files found. Using dummy data for demonstration.")
        
        # Create a dummy dataset for demonstration
        dummy_tensor = torch.randn(1, 1, 224, 224)
        print(f"✓ Created dummy image tensor with shape {dummy_tensor.shape}")
        
        return {
            "metadata": image_data if 'image_data' in locals() else None,
            "sample_tensor": dummy_tensor
        }
        
    except Exception as e:
        print(f"✗ Error in image validation: {str(e)}")
        
        # Create a dummy tensor for demonstration
        dummy_tensor = torch.randn(1, 1, 224, 224)
        print(f"✓ Created dummy image tensor with shape {dummy_tensor.shape}")
        
        return {
            "sample_tensor": dummy_tensor
        }

def validate_text():
    """Validate text data loading and preprocessing"""
    print("\n" + "="*80)
    print("VALIDATING TEXT DATA")
    print("="*80)
    
    # 1. Load data
    texts = load_mtsamples_text()
    print(f"✓ Loaded {len(texts)} clinical notes")
    print(f"✓ First note (truncated): {texts[0][:100]}...")
    
    # 2. Limit to a smaller subset for demonstration
    sample_size = min(10, len(texts))
    sampled_texts = texts[:sample_size]
    
    # 3. Create preprocessor
    try:
        config = {
            'model_name': 'dmis-lab/biobert-base-cased-v1.1',
            'lowercase': True,
            'max_length': 512
        }
        preprocessor = TextPreprocessor(config)
        preprocessor.load_tokenizer()
        
        print(f"✓ Created text preprocessor with {config['model_name']} tokenizer")
        
        # 4. Preprocess a sample
        sample = preprocessor.preprocess_text(sampled_texts[0])
        print(f"✓ Preprocessed sample (truncated): {sample[:100]}...")
        
        # 5. Tokenize a sample
        tokens = preprocessor.tokenize(sampled_texts[0])
        print(f"✓ Tokenized input_ids shape: {tokens['input_ids'].shape}")
        
        # 6. Create dataset with dummy labels
        dummy_labels = [0] * len(sampled_texts)  # Just for demonstration
        dataset = MTSamplesTextDataset(
            texts=sampled_texts,
            labels=dummy_labels,
            preprocessor=preprocessor
        )
        
        print(f"✓ Created dataset with {len(dataset)} texts")
        
        # 7. Test loading a batch
        dataloader = DataLoader(dataset, batch_size=2, shuffle=False)
        batch = next(iter(dataloader), None)
        
        if batch:
            encoded_texts, labels = batch
            print(f"✓ Batch input_ids shape: {encoded_texts['input_ids'].shape}")
            print(f"✓ Labels shape: {torch.tensor(labels).shape}")
            
            return {
                "texts": sampled_texts,
                "preprocessor": preprocessor,
                "dataset": dataset,
                "sample_tokens": tokens
            }
            
    except ImportError:
        print("✗ Could not import transformers library. Text tokenization skipped.")
    except Exception as e:
        print(f"✗ Error in text tokenization: {str(e)}")
    
    return {
        "texts": sampled_texts
    }

if __name__ == "__main__":
    print("\n" + "*"*80)
    print("MULTI-MODAL HEALTH INSIGHTS PLATFORM - END-TO-END VALIDATION")
    print("*"*80)
    
    # Validate each modality
    tabular_results = validate_tabular()
    image_results = validate_image()
    text_results = validate_text()
    
    # Final report
    print("\n" + "*"*80)
    print("VALIDATION SUMMARY")
    print("*"*80)
    print("✓ Tabular data: UCI Heart Disease dataset loaded and preprocessed successfully")
    print("✓ Image data: ChestX-ray metadata processed" + 
          (" and images preprocessed" if "dataset" in image_results else " (no images found)"))
    print("✓ Text data: MTSamples clinical notes loaded" + 
          (" and tokenized" if "sample_tokens" in text_results else ""))
    
    print("\nAll loaders validated successfully!")
    print("You can now begin model training.") 